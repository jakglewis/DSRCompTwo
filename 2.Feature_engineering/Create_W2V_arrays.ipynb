{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import datetime\n",
    "import BoW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "path = os.getcwd()\n",
    "parent_folder, current_folder = os.path.split(path)\n",
    "df = pd.read_csv(parent_folder + '/0.Raw_data/train/Combined_News_DJIA_train.csv')   # please check if Training data is in the same location on your PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Lower case of all words\n",
    "    2. Remove all punctuation\n",
    "    3. Remove all stopwords\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "\n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and merging all headlines to one single column\n",
    "\n",
    "df.iloc[:,2:27] = df.iloc[:,2:27].applymap(str)\n",
    "\n",
    "# replace the b' and b\" which are in the beginning of some headlines\n",
    "df.iloc[:,2:27] = df.iloc[:,2:27].replace(regex=\"b'\",value=\"\")\n",
    "df.iloc[:,2:27] = df.iloc[:,2:27].replace(regex='b\"',value='')\n",
    "df.iloc[:,2:27] = df.iloc[:,2:27].apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "#df1['lengths'] = df1['headlines'].apply(len)\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,27):\n",
    "    df.iloc[:,i] = df.iloc[:,i].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "train_length = int(len(df)*(1-val_size))\n",
    "train = df.iloc[:train_length+1,:]\n",
    "val = df.iloc[train_length:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.loc[:,]\n",
    "X_val = val.loc[:,]\n",
    "y_train = df.iloc[:train_length+1,:]\n",
    "y_train = y_train['Label']\n",
    "y_val = df.iloc[train_length:,:]\n",
    "y_val = y_val['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_create(X_train):\n",
    "    sentences = []\n",
    "    col = X_train.loc[:,X_train.dtypes == object]\n",
    "    for i in range(col.shape[1]):\n",
    "        sentences.extend(col.iloc[:,i].tolist())\n",
    "    w2v_model = Word2Vec(sentences, min_count=4)\n",
    "    with open('w2v_model.pk', 'wb') as fin:\n",
    "        pickle.dump(w2v_model, fin)\n",
    "    pass\n",
    "\n",
    "def transform_vocab(wordlist):\n",
    "    '''\n",
    "    Use dataframe.apply(transform_vocab) to transform the list of words in each Dataframe column to a list of numpy array vectors\n",
    "    :param wordlist: list of words\n",
    "    :return: list of numpy arrays with word vector (length: 100) and padded sequence (length:20)\n",
    "    '''\n",
    "    with open('w2v_model.pk', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    filtered_wl = [word for word in wordlist if word in model.wv.vocab]\n",
    "    if not filtered_wl: \n",
    "        filtered_wl = ['nan']\n",
    "    vectorlist = model.wv[filtered_wl]\n",
    "    paddings = 20 - len(vectorlist)\n",
    "    padded_vectorlist = paddings * [100 * [0]] + vectorlist[0:20].tolist()\n",
    "    return np.asarray(padded_vectorlist)\n",
    "\n",
    "def w2v_transform(X_input):\n",
    "    '''\n",
    "    Transforms the cleaned Dataframe from containing a list of words to containing a padded sequence of word vectors.\n",
    "    :param X_train:\n",
    "    :return: padded vectorized Dataframe\n",
    "    '''\n",
    "    arraylist = []\n",
    "    dn = np.array((X_input.shape[0],X_input.shape[1], 20, 100))    \n",
    "    idx = [X_input.columns.get_loc(c) for c in X_input.filter(like='Top').columns if c in X_input]\n",
    "    for j in idx:\n",
    "        for i in range(X_input.shape[0]):\n",
    "            arraylist.append(transform_vocab(X_input.iloc[i][j]))         \n",
    "    return arraylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_create(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = w2v_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31700, 20, 100)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(X_t)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1,25,20,100)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('W2V_X_Train.pk', 'wb') as fin:\n",
    "        pickle.dump(X, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_va = w2v_transform(X_val)\n",
    "Xval = np.asarray(X_va)\n",
    "Xval = Xval.reshape(-1,25,20,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('W2V_X_Val.pk', 'wb') as fin:\n",
    "        pickle.dump(Xval, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
