{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import datetime\n",
    "from pandas import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('DSRCompTwo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Open          High           Low         Close     Volume  \\\n",
      "Date                                                                            \n",
      "2008-08-08  11432.089844  11759.959961  11388.040039  11734.320312  212830000   \n",
      "2008-08-11  11729.669922  11867.110352  11675.530273  11782.349609  183190000   \n",
      "2008-08-12  11781.700195  11782.349609  11601.519531  11642.469727  173590000   \n",
      "2008-08-13  11632.809570  11633.780273  11453.339844  11532.959961  182550000   \n",
      "2008-08-14  11532.070312  11718.280273  11450.889648  11615.929688  159790000   \n",
      "\n",
      "               Adj Close  \n",
      "Date                      \n",
      "2008-08-08  11734.320312  \n",
      "2008-08-11  11782.349609  \n",
      "2008-08-12  11642.469727  \n",
      "2008-08-13  11532.959961  \n",
      "2008-08-14  11615.929688  \n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "df = pd.read_csv('0.Raw_data/train/Combined_News_DJIA_train.csv')   # please check if Training data is in the same location on your PC\n",
    "df = df.set_index(\"Date\")\n",
    "def parser(x):\n",
    "    return datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "series = pd.read_csv('0.Raw_data/DJIA_table.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "series = series.sort_index(ascending=True)\n",
    "print(series.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "series['lag_adj_close'] = series['Adj Close'].shift(1)\n",
    "series['diff_close'] = series['Adj Close']-series['lag_adj_close']\n",
    "series['target'] = 0\n",
    "series['target'] = (series['diff_close']>=0 ).map(int)\n",
    "#series['target'] = diff\n",
    "series['target']\n",
    "series_new = series.loc[:'2014-11-20']\n",
    "new_df=pd.concat([series_new['target'],df['Label']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label']  = series_new['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2008-08-08', '2008-08-11', '2008-08-12', '2008-08-13', '2008-08-14',\n",
       "       '2008-08-15', '2008-08-18', '2008-08-19', '2008-08-20', '2008-08-21',\n",
       "       ...\n",
       "       '2014-11-07', '2014-11-10', '2014-11-11', '2014-11-12', '2014-11-13',\n",
       "       '2014-11-14', '2014-11-17', '2014-11-18', '2014-11-19', '2014-11-20'],\n",
       "      dtype='object', name='Date', length=1584)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and merging all headlines to one single column\n",
    "\n",
    "subdf = df.iloc[:,2:27] \n",
    "subdf = subdf.applymap(str)\n",
    "s = subdf.apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# replace the b' and b\" which are in the beginning of some headlines\n",
    "s = s.str.replace(\"b'\",\"\")\n",
    "s = s.str.replace('b\"','')\n",
    "\n",
    "# create a new dataframe with all headlines and the overall word count\n",
    "df1 = s.to_frame(name='headlines')\n",
    "df1['lengths'] = df1['headlines'].apply(len)\n",
    "df1['date'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1584"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "train_length = int(len(df1)*(1-val_size))\n",
    "train = df1.iloc[:train_length+1,:]\n",
    "val = df1.iloc[train_length:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268\n",
      "317\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(val))\n",
    "#split dataset base don time\n",
    "X_train = train.loc[:,]\n",
    "X_val = val.loc[:,]\n",
    "y_train = df.iloc[:train_length+1,:]\n",
    "y_train=y_train['Label']\n",
    "y_val = df.iloc[train_length:,:]\n",
    "y_val=y_val['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "\n",
    "\n",
    "# create function to clean the text\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Lower case of all words\n",
    "    2. Remove all punctuation\n",
    "    3. Remove all stopwords\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # transforms all to lower case words\n",
    "    mess = mess.lower()\n",
    "\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "\n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word not in stopwords.words('english')]\n",
    "\n",
    "# create a WordNet tokenizer\n",
    "wrd_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def wn_tokenizer(doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    return wrd_lemmatizer.lemmatize(tokens, pos='v')\n",
    "\n",
    "def bow_encoding(s):\n",
    "    '''\n",
    "    Takes a pandas Series and fits and transforms the strings to a Bag of Words matrix\n",
    "    :param s: pandas series\n",
    "    :return: Numpy Matrix\n",
    "    '''\n",
    "\n",
    "    BoW_WN = CountVectorizer(analyzer=text_process, tokenizer=wn_tokenizer, min_df=4)\n",
    "    BoW_WN_matrix = BoW_WN.fit_transform(s)\n",
    "    with open('CountVectorizer.pk', 'wb') as fin:\n",
    "        pickle.dump(BoW_WN, fin)\n",
    "    return BoW_WN_matrix\n",
    "\n",
    "def bow_encoding_val(s):\n",
    "    '''\n",
    "    Takes a pandas Series and transforms the strings to a Bag of Words matrix\n",
    "    :param s: pandas series\n",
    "    :return: Numpy Matrix\n",
    "    '''\n",
    "\n",
    "    with open('CountVectorizer.pk', 'rb') as f:\n",
    "        BoW_WN = pickle.load(f)\n",
    "    BoW_WN_matrix = BoW_WN.transform(s)\n",
    "\n",
    "    return BoW_WN_matrix\n",
    "\n",
    "\n",
    "def tfidf_encoding(s):\n",
    "    '''\n",
    "    Takes a series of Texts and fit and transforms the strings to a TFIDF matrix\n",
    "    :param s: pandas series\n",
    "    :return: numpy matrix\n",
    "    '''\n",
    "\n",
    "    tfidf_vectorizer_WN = TfidfVectorizer(analyzer=text_process, use_idf=True, tokenizer=wn_tokenizer, min_df=4)\n",
    "    tfidf_WN_matrix = tfidf_vectorizer_WN.fit_transform(s)\n",
    "    with open('TFIDFVectorizer.pk', 'wb') as fin:\n",
    "        pickle.dump(tfidf_vectorizer_WN, fin)\n",
    "    return tfidf_WN_matrix\n",
    "\n",
    "\n",
    "def tfidf_encoding_val(s):\n",
    "    '''\n",
    "    Takes a series of Texts and transforms the strings to a TFIDF matrix\n",
    "    :param s: pandas series\n",
    "    :return: Numpy Matrix\n",
    "    '''\n",
    "\n",
    "    with open('TFIDFVectorizer.pk', 'rb') as f:\n",
    "        tfidf_vectorizer_WN = pickle.load(f)\n",
    "    tfidf_WN_matrix = tfidf_vectorizer_WN.transform(s)\n",
    "\n",
    "    return tfidf_WN_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1268x9967 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 268532 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the training data set\n",
    "df_encode = bow_encoding(X_train['headlines'])\n",
    "df_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<317x9967 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 68758 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming\n",
    "#from BoW import bow_encoding_val\n",
    "df_encode_val = bow_encoding_val(X_val['headlines'])\n",
    "df_encode_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1268x9967 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 268532 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encode_TFIDF = tfidf_encoding(X_train['headlines'])\n",
    "df_encode_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<317x9967 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 68758 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encode_TFIDF_val = tfidf_encoding_val(X_val['headlines'])\n",
    "df_encode_TFIDF_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow_train= pd.DataFrame(df_encode.toarray())\n",
    "X_bow_val= pd.DataFrame(df_encode_val.toarray())\n",
    "X_TFIDF_train= pd.DataFrame(df_encode_TFIDF.toarray())\n",
    "X_TFIDF_val= pd.DataFrame(df_encode_TFIDF_val.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = y_train\n",
    "Y_val = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lundr\\\\DSRCompTwo'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "#os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow_train.to_pickle('4.Data/X_bow_train_v3.pkl')\n",
    "X_bow_val.to_pickle('4.Data/X_bow_val_v3.pkl')\n",
    "X_TFIDF_train.to_pickle('4.Data/X_TFIDF_train_v3.pkl')\n",
    "X_TFIDF_val.to_pickle('4.Data/X_TFIDF_val_v3.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.to_pickle('4.Data/Y_train_v3.pkl')\n",
    "Y_val.to_pickle('4.Data/Y_val_v3.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
